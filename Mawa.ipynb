{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2869e8e-ac33-49d2-a233-6bbd481f9cac",
   "metadata": {},
   "source": [
    "# Mawa Scraping:\n",
    "This notebook demonstrates step-by-step web scraping from mawa., from setup and request to parsing, data extraction, and saving results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d8ce6046-c9ea-42d5-b655-be88eff87506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import requests\n",
    "import csv\n",
    "import time\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3173756c-f26f-4b13-95a1-9ff19bcab629",
   "metadata": {},
   "source": [
    "# 1.CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7e907ee3-38f6-4595-9407-4ee2db33e15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PAGE       = \"https://www.mawa.om/en/rent\"\n",
    "LISTING_API     = \"https://www.mawa.om/en/PropertyListing\"\n",
    "OUTPUT_FILE     = \"mawa_rent_listings.csv\"\n",
    "DESIRED_RECORDS = 1000\n",
    "PER_PAGE        = 50        \n",
    "SLEEP_BETWEEN   = 0.1       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661003d2-5c1f-4d1b-90ac-0dd945728d9e",
   "metadata": {},
   "source": [
    "# 2.start a session & pretend to be Chrome "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0863d9e9-57a8-40cc-be22-bfab60704255",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = requests.Session()\n",
    "session.headers.update({\n",
    "    \"User-Agent\": (\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "        \"Chrome/138.0.7204.101 Safari/537.36\"\n",
    "    ),\n",
    "    \"Accept\": (\n",
    "        \"text/html,application/xhtml+xml,application/xml;\"\n",
    "        \"q=0.9,image/webp,image/apng,*/*;q=0.8\"\n",
    "    ),\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "    \"Connection\": \"keep-alive\",\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f18730-0722-48ea-8612-8c5cdee6df01",
   "metadata": {},
   "source": [
    "# 3.GET the rent page HTML for cookies + CSRF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d79c7bca-a385-4725-bafe-c1a7f4e29e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = session.get(BASE_PAGE)\n",
    "r.raise_for_status()  # should now succeed\n",
    "soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "csrf = soup.select_one('meta[name=\"csrf-token\"]')[\"content\"]\n",
    "\n",
    "# prepare headers for the AJAX call \n",
    "session.headers.update({\n",
    "    \"Accept\": \"text/html, */*; q=0.01\",\n",
    "    \"X-Requested-With\": \"XMLHttpRequest\",\n",
    "    \"X-CSRF-TOKEN\": csrf,\n",
    "    \"Referer\": BASE_PAGE,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f827532-f0e0-4f47-aee8-cffc1d303825",
   "metadata": {},
   "source": [
    "# 4.fetch one page of listing‐cards HTML "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1d03546c-9d31-4797-870d-136be78f4ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_page(page_num):\n",
    "    form = {\n",
    "        \"user_type\":       \"\",\n",
    "        \"furnished_type\":  \"\",\n",
    "        \"sort_by\":         \"newest\",\n",
    "        \"location\":        \"\",\n",
    "        \"min_area\":        \"\",\n",
    "        \"max_area\":        \"\",\n",
    "        \"min_price\":       \"\",\n",
    "        \"max_price\":       \"\",\n",
    "        \"amenities_id\":    \"\",\n",
    "        \"bed\":             \"\",\n",
    "        \"bath\":            \"\",\n",
    "        \"property_type\":   \"\",\n",
    "        \"keyword\":         \"\",\n",
    "        \"property_for\":    3,      # 3 = rent\n",
    "        \"page\":            page_num,\n",
    "        \"_token\":          csrf,\n",
    "    }\n",
    "    resp = session.post(LISTING_API, data=form, timeout=10)\n",
    "    resp.raise_for_status()\n",
    "    return resp.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4285fe49-ded5-430d-9918-b8691afe7ba6",
   "metadata": {},
   "source": [
    "# 5.parse the HTML fragment into Python dicts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "52b1036a-f9de-418a-b9d9-9f5f3293c01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_listings(html_fragment):\n",
    "    frag_soup = BeautifulSoup(html_fragment, \"html.parser\")\n",
    "    cards = frag_soup.select(\"div.col-sm-6.col-lg-4\")\n",
    "    out = []\n",
    "    for card in cards:\n",
    "        a = card.select_one(\".inner_feature_cntnt_heading a\")\n",
    "        if not a:\n",
    "            continue\n",
    "\n",
    "        href = a[\"href\"].strip()\n",
    "        url  = href if href.startswith(\"http\") else f\"https://www.mawa.om{href}\"\n",
    "        title = a.find([\"h2\",\"h3\"]).get_text(strip=True) if a.find([\"h2\",\"h3\"]) else \"\"\n",
    "\n",
    "        loc_el   = card.select_one(\".view_map_flag p\")\n",
    "        location = loc_el.get_text(strip=True) if loc_el else \"\"\n",
    "\n",
    "        price_el = card.select_one(\".prop_img .price p\") or card.select_one(\".price p\")\n",
    "        price    = price_el.get_text(strip=True) if price_el else \"\"\n",
    "\n",
    "        beds_el  = card.select_one(\"i.fa-bed\")\n",
    "        bedrooms = beds_el.parent.get_text(strip=True) if beds_el and beds_el.parent else \"\"\n",
    "\n",
    "        baths_el   = card.select_one(\"i.fa-bath\")\n",
    "        bathrooms  = baths_el.parent.get_text(strip=True) if baths_el and baths_el.parent else \"\"\n",
    "\n",
    "        area_el = card.select_one(\"i.fa-area-chart\")\n",
    "        area    = area_el.parent.get_text(strip=True) if area_el and area_el.parent else \"\"\n",
    "\n",
    "        type_el = card.select_one(\"i.fa-building-o\")\n",
    "        ptype   = type_el.parent.get_text(strip=True) if type_el and type_el.parent else \"\"\n",
    "\n",
    "        out.append({\n",
    "            \"URL\":           url,\n",
    "            \"Title\":         title,\n",
    "            \"Location\":      location,\n",
    "            \"Price\":         price,\n",
    "            \"Bedrooms\":      bedrooms,\n",
    "            \"Bathrooms\":     bathrooms,\n",
    "            \"Area\":          area,\n",
    "            \"PropertyType\":  ptype\n",
    "        })\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270be026-34c7-4a43-bb37-0273e0cfe689",
   "metadata": {},
   "source": [
    "# 6.loop pages until we have enough "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d76774c6-defa-47d9-88d4-86eee0e611a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching page 1… (have 0)\n",
      "Fetching page 2… (have 12)\n",
      "Fetching page 3… (have 24)\n",
      "Fetching page 4… (have 36)\n",
      "Fetching page 5… (have 48)\n",
      "Fetching page 6… (have 60)\n",
      "Fetching page 7… (have 72)\n",
      "Fetching page 8… (have 84)\n",
      "Fetching page 9… (have 96)\n",
      "Fetching page 10… (have 108)\n",
      "Fetching page 11… (have 120)\n",
      "Fetching page 12… (have 132)\n",
      "Fetching page 13… (have 144)\n",
      "Fetching page 14… (have 156)\n",
      "Fetching page 15… (have 168)\n",
      "Fetching page 16… (have 180)\n",
      "Fetching page 17… (have 192)\n",
      "Fetching page 18… (have 204)\n",
      "Fetching page 19… (have 216)\n",
      "Fetching page 20… (have 228)\n",
      "Fetching page 21… (have 240)\n",
      "Fetching page 22… (have 252)\n",
      "Fetching page 23… (have 264)\n",
      "Fetching page 24… (have 276)\n",
      "Fetching page 25… (have 288)\n",
      "Fetching page 26… (have 300)\n",
      "Fetching page 27… (have 312)\n",
      "Fetching page 28… (have 324)\n",
      "Fetching page 29… (have 336)\n",
      "Fetching page 30… (have 348)\n",
      "Fetching page 31… (have 360)\n",
      "Fetching page 32… (have 372)\n",
      "Fetching page 33… (have 384)\n",
      "Fetching page 34… (have 396)\n",
      "Fetching page 35… (have 408)\n",
      "Fetching page 36… (have 420)\n",
      "Fetching page 37… (have 432)\n",
      "Fetching page 38… (have 444)\n",
      "Fetching page 39… (have 456)\n",
      "Fetching page 40… (have 468)\n",
      "Fetching page 41… (have 480)\n",
      "Fetching page 42… (have 492)\n",
      "Fetching page 43… (have 504)\n",
      "Fetching page 44… (have 516)\n",
      "Fetching page 45… (have 528)\n",
      "Fetching page 46… (have 540)\n",
      "Fetching page 47… (have 552)\n",
      "Fetching page 48… (have 564)\n",
      "Fetching page 49… (have 576)\n",
      "Fetching page 50… (have 588)\n",
      "Fetching page 51… (have 600)\n",
      "Fetching page 52… (have 612)\n",
      "Fetching page 53… (have 624)\n",
      "Fetching page 54… (have 636)\n",
      "Fetching page 55… (have 648)\n",
      "Fetching page 56… (have 660)\n",
      "Fetching page 57… (have 672)\n",
      "Fetching page 58… (have 684)\n",
      "Fetching page 59… (have 696)\n",
      "Fetching page 60… (have 708)\n",
      "Fetching page 61… (have 720)\n",
      "Fetching page 62… (have 732)\n",
      "Fetching page 63… (have 744)\n",
      "Fetching page 64… (have 756)\n",
      "Fetching page 65… (have 768)\n",
      "Fetching page 66… (have 780)\n",
      "Fetching page 67… (have 792)\n",
      "Fetching page 68… (have 804)\n",
      "Fetching page 69… (have 816)\n",
      "Fetching page 70… (have 828)\n",
      "Fetching page 71… (have 840)\n",
      "Fetching page 72… (have 852)\n",
      "Fetching page 73… (have 864)\n",
      "Fetching page 74… (have 876)\n",
      "Fetching page 75… (have 888)\n",
      "Fetching page 76… (have 900)\n",
      "Fetching page 77… (have 912)\n",
      "Fetching page 78… (have 924)\n",
      "Fetching page 79… (have 936)\n",
      "Fetching page 80… (have 948)\n",
      "Fetching page 81… (have 960)\n",
      "Fetching page 82… (have 972)\n",
      "Fetching page 83… (have 984)\n",
      "Fetching page 84… (have 996)\n"
     ]
    }
   ],
   "source": [
    "all_listings = []\n",
    "page = 1\n",
    "while len(all_listings) < DESIRED_RECORDS:\n",
    "    print(f\"Fetching page {page}… (have {len(all_listings)})\")\n",
    "    fragment = fetch_page(page)\n",
    "    listings = parse_listings(fragment)\n",
    "    if not listings:\n",
    "        print(\"No more listings returned—stopping early.\")\n",
    "        break\n",
    "\n",
    "    all_listings.extend(listings)\n",
    "    if len(all_listings) >= DESIRED_RECORDS:\n",
    "        break\n",
    "\n",
    "    page += 1\n",
    "    time.sleep(SLEEP_BETWEEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76792abb-5c13-40fc-9985-60113dfac5af",
   "metadata": {},
   "source": [
    "# 7.trim to EXACT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1c932d41-a8b1-41d9-88d9-e59aa9bc63d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 1000 listings total.\n"
     ]
    }
   ],
   "source": [
    "all_listings = all_listings[:DESIRED_RECORDS]\n",
    "print(f\"Collected {len(all_listings)} listings total.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c03bd0-c6aa-4b41-9ff3-b53996567628",
   "metadata": {},
   "source": [
    "# 8.SAVE TO CSV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91a5973-03e5-493f-ad18-f138fd96d245",
   "metadata": {},
   "outputs": [],
   "source": [
    "if all_listings:\n",
    "    keys = list(all_listings[0].keys())\n",
    "    with open(OUTPUT_FILE, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=keys)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(all_listings)\n",
    "    print(f\" Saved {len(all_listings)} records to '{OUTPUT_FILE}'\")\n",
    "else:\n",
    "    print(\" No data scraped—check if the site structure changed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
